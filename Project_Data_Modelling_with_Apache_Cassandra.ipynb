{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Modelling with Apache Cassandra</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "from cassandra.cluster import Cluster\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of file paths for each original event CSV data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace/home\n",
      "\n",
      "The follwing files are included:\n",
      "event_data\n",
      "Project_1B_ Project_Template.ipynb\n",
      "images\n",
      ".workspace-config.json\n",
      "event_datafile_new.csv\n",
      ".ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Check the current working directory\n",
    "print('Current Directory:', os.getcwd())\n",
    "\n",
    "# Print the name of files sitting in current working directory\n",
    "print('\\nThe follwing files are included:')\n",
    "for file in os.listdir():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/home/event_data/2018-11-13-events.csv', '/workspace/home/event_data/2018-11-23-events.csv', '/workspace/home/event_data/2018-11-19-events.csv', '/workspace/home/event_data/2018-11-16-events.csv', '/workspace/home/event_data/2018-11-20-events.csv', '/workspace/home/event_data/2018-11-27-events.csv', '/workspace/home/event_data/2018-11-11-events.csv', '/workspace/home/event_data/2018-11-05-events.csv', '/workspace/home/event_data/2018-11-03-events.csv', '/workspace/home/event_data/2018-11-09-events.csv', '/workspace/home/event_data/2018-11-01-events.csv', '/workspace/home/event_data/2018-11-30-events.csv', '/workspace/home/event_data/2018-11-29-events.csv', '/workspace/home/event_data/2018-11-22-events.csv', '/workspace/home/event_data/2018-11-17-events.csv', '/workspace/home/event_data/2018-11-24-events.csv', '/workspace/home/event_data/2018-11-26-events.csv', '/workspace/home/event_data/2018-11-10-events.csv', '/workspace/home/event_data/2018-11-02-events.csv', '/workspace/home/event_data/2018-11-12-events.csv', '/workspace/home/event_data/2018-11-21-events.csv', '/workspace/home/event_data/2018-11-25-events.csv', '/workspace/home/event_data/2018-11-14-events.csv', '/workspace/home/event_data/2018-11-08-events.csv', '/workspace/home/event_data/2018-11-06-events.csv', '/workspace/home/event_data/2018-11-04-events.csv', '/workspace/home/event_data/2018-11-18-events.csv', '/workspace/home/event_data/2018-11-07-events.csv', '/workspace/home/event_data/2018-11-15-events.csv', '/workspace/home/event_data/2018-11-28-events.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get the file path to event data\n",
    "filepath = os.path.join(os.getcwd(), 'event_data')\n",
    "\n",
    "# Collect and print the file path of each event CSV file\n",
    "files_list = glob.glob(os.path.join(filepath, '*.csv'))\n",
    "            \n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the individual event files to consolidate data into a single streamlined CSV file for modelling in Apache Casssandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist,auth,firstName,gender,itemInSession,lastName,length,level,location,method,page,registration,sessionId,song,status,ts,userId\r",
      "\r\n",
      ",Logged In,Kevin,M,0,Arellano,,free,\"Harrisburg-Carlisle, PA\",GET,Home,1.54001E+12,514,,200,1.54207E+12,66\r",
      "\r\n",
      "Fu,Logged In,Kevin,M,1,Arellano,280.05832,free,\"Harrisburg-Carlisle, PA\",PUT,NextSong,1.54001E+12,514,Ja I Ty,200,1.54207E+12,66\r",
      "\r\n",
      ",Logged In,Maia,F,0,Burke,,free,\"Houston-The Woodlands-Sugar Land, TX\",GET,Home,1.54068E+12,510,,200,1.54207E+12,51\r",
      "\r\n",
      "All Time Low,Logged In,Maia,F,1,Burke,177.84118,free,\"Houston-The Woodlands-Sugar Land, TX\",PUT,NextSong,1.54068E+12,510,A Party Song (The Walk of Shame),200,1.54207E+12,51\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Check a sample CSV file to see the first 5 rows\n",
    "!head -n 5 '/workspace/home/event_data/2018-11-13-events.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold all data rows\n",
    "data_rows = []\n",
    "# Initialize an empty list to hold the headers\n",
    "headers = []\n",
    "\n",
    "# Iterate through each file in the list of files\n",
    "for file in files_list:\n",
    "    # Open the current file in read mode with UTF-8 encoding\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        # Read all rows from the CSV file into a list\n",
    "        rows = list(csv.reader(csvfile, delimiter=',', quotechar='\"'))\n",
    "        # If headers have not been set yet, set them to the first row of the current file\n",
    "        if not headers:\n",
    "            headers = rows[0]\n",
    "        # Iterate through all rows except the header row\n",
    "        for row in rows[1:]:\n",
    "            # Check if the first element of the row is not an empty string\n",
    "            if row[0]:\n",
    "                # If not empty, append the row to the data_rows list\n",
    "                data_rows.append(row)\n",
    "\n",
    "# Open a new file in write mode with UTF-8 encoding to save the combined data\n",
    "with open(\n",
    "    'event_datafile_new.csv',\n",
    "    'w',\n",
    "    newline='',\n",
    "    encoding='utf-8',\n",
    ") as newfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(\n",
    "        newfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    # Write the headers to the new file\n",
    "    writer.writerow(headers)\n",
    "    # Write each data row to the new file\n",
    "    for row in data_rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist,auth,firstName,gender,itemInSession,lastName,length,level,location,method,page,registration,sessionId,song,status,ts,userId\r",
      "\r\n",
      "Fu,Logged In,Kevin,M,1,Arellano,280.05832,free,\"Harrisburg-Carlisle, PA\",PUT,NextSong,1.54001E+12,514,Ja I Ty,200,1.54207E+12,66\r",
      "\r\n",
      "All Time Low,Logged In,Maia,F,1,Burke,177.84118,free,\"Houston-The Woodlands-Sugar Land, TX\",PUT,NextSong,1.54068E+12,510,A Party Song (The Walk of Shame),200,1.54207E+12,51\r",
      "\r\n",
      "Nik & Jay,Logged In,Wyatt,M,0,Scott,196.51873,free,\"Eureka-Arcata-Fortuna, CA\",PUT,NextSong,1.54087E+12,379,Pop-Pop!,200,1.54208E+12,9\r",
      "\r\n",
      "Quad City DJ's,Logged In,Chloe,F,0,Cuevas,451.44771,free,\"San Francisco-Oakland-Hayward, CA\",PUT,NextSong,1.54094E+12,506,C'mon N' Ride It (The Train) (LP Version),200,1.54208E+12,49\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 5 rows of the consolidated event file\n",
    "!head -n 5 'event_datafile_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userId']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the headers of the CSV file\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows in the consolidated event file\n",
    "with open('event_datafile_new.csv', 'r', encoding='utf-8') as file:\n",
    "    print(\n",
    "        len(file.readlines())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Data Modelling in Apache Cassandra\n",
    "\n",
    "**Now the CSV file titled <font color=red>event_datafile_new.csv</font> is ready to work with, located within the current working directory.  The <font color=red>event_datafile_new.csv</font> contains the following columns:**\n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data appears like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "![image](https://github.com/user-attachments/assets/5a2cd3d4-b1d1-4052-837b-8e7ade421e31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to the Cassandra instance on the local machine\n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    # Establish a session with the Cassandra cluster\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Execute a CQL command to create a new keyspace called \"sparkifydb\"\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS sparkifydb\n",
    "        WITH REPLICATION = {\n",
    "            'class': 'SimpleStrategy',\n",
    "            'replication_factor': 1\n",
    "        }\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set the active keyspace for the session to 'sparkifydb'\n",
    "    session.set_keyspace('sparkifydb')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create queries to ask the following three questions of the data\n",
    "\n",
    "**1. Get the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession = 4**\n",
    "\n",
    "```SQL\n",
    "SELECT artist, song, length \n",
    "FROM song_table \n",
    "WHERE sessionId = 338 AND itemInSession = 4\n",
    "```\n",
    "\n",
    "\n",
    "**2. Get only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182**\n",
    "    \n",
    "```SQL\n",
    "SELECT artist, song, user\n",
    "FROM artist_table \n",
    "WHERE userId = 10 AND sessionId = 182\n",
    "```\n",
    "\n",
    "**3. Get every user name (first and last) in the music app history who listened to the song 'All Hands Against His Own'**\n",
    "\n",
    "```SQL\n",
    "SELECT user\n",
    "FROM user_table\n",
    "WHERE song = 'All Hands Against His Own'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling for Query 1\n",
    "Get the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "```SQL\n",
    "SELECT artist, song, length \n",
    "FROM song_table \n",
    "WHERE sessionId = 338 AND itemInSession = 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be **partitioned** by `sessionId` with a **clustering** column of `itemInSession`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Table named `song_table` based on the Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create song_table for Query 1\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS song_table (\n",
    "            sessionId            int,\n",
    "            itemInSession        int,\n",
    "            artist               text,\n",
    "            song                 text,\n",
    "            length               float,\n",
    "            PRIMARY KEY ((sessionId), itemInSession)\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data into `song_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded to the song_table!\n"
     ]
    }
   ],
   "source": [
    "# Specify the columns we want to extract from the CSV file\n",
    "target_cols = ['sessionId', 'itemInSession', 'artist', 'song', 'length']\n",
    "\n",
    "# Define the SQL INSERT query to insert data into the 'song_table'\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO song_table (sessionId, itemInSession, \n",
    "                            artist, song, length)\n",
    "    VALUES\n",
    "    (%s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Open the consolidated CSV file in read mode with UTF-8 encoding\n",
    "with open('event_datafile_new.csv', 'r', encoding='utf-8') as file:\n",
    "    # Read all rows from the CSV file into a list\n",
    "    rows = list(csv.reader(file, delimiter=',', quotechar='\"'))\n",
    "    \n",
    "    # Extract the headers (column names) from the first row\n",
    "    headers = rows[0]\n",
    "\n",
    "    # Find the indices of the target columns within the headers\n",
    "    target_col_indices = [headers.index(col) for col in target_cols]\n",
    "\n",
    "    # Iterate over each row in the CSV file, starting from the second row\n",
    "    for row in rows[1:]:\n",
    "        # Extract the relevant column values from each row using the indices\n",
    "        sessionId, itemInSession, artist, song, length = [\n",
    "            row[i] for i in target_col_indices\n",
    "        ]\n",
    "\n",
    "        # Execute the SQL INSERT query with the extracted values\n",
    "        session.execute(\n",
    "            insert_query,(\n",
    "                int(sessionId), \n",
    "                int(itemInSession), \n",
    "                artist, \n",
    "                song, \n",
    "                float(length),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print a message indicating that data loading is complete\n",
    "    print(f'Data successfully loaded to the song_table!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a SELECT query to confirm that the data has been successfully inserted into the `song_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Faithless', 'Music Matters (Mark Knight Dub)', 495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "# Run a SELECT query to verify the data was loaded into the table\n",
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT artist, song, length\n",
    "        FROM song_table\n",
    "        WHERE sessionId = 338\n",
    "        AND itemInSession = 4\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print((row.artist, row.song, row.length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling for Query 2\n",
    "\n",
    "Get only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "```SQL\n",
    "SELECT artist, song, user \n",
    "FROM artist_table \n",
    "WHERE userId = 10 AND sessionId = 182\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `song` column is to be sorted by `itemInSession`, we should **partition** data using a **composite partition key** of `userId` and `sessionId`. This way, within each partition, the data will be sorted by the **clustering** column `itemInSession`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Table named `artist_table` based on the Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artist_table for Query 2\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS artist_table (\n",
    "            userId            int,\n",
    "            sessionId         int,\n",
    "            itemInSession     int,\n",
    "            song              text,\n",
    "            artist            text,\n",
    "            user              text,\n",
    "            PRIMARY KEY ((userId, sessionId), itemInSession)\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data into `artist_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded to the artist_table!\n"
     ]
    }
   ],
   "source": [
    "# Specify the columns we want to extract from the CSV file\n",
    "target_cols = ['userId', 'sessionId', 'itemInSession', \n",
    "               'song', 'artist', 'firstName', 'lastName']\n",
    "\n",
    "# Define the SQL INSERT query to insert data into the 'artist_table'\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO artist_table (userId, sessionId, itemInSession, \n",
    "                              song, artist, user)\n",
    "    VALUES\n",
    "    (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Open the consolidated CSV file in read mode with UTF-8 encoding\n",
    "with open('event_datafile_new.csv', 'r', encoding='utf-8') as file:\n",
    "    # Read all rows from the CSV file into a list\n",
    "    rows = list(csv.reader(file, delimiter=',', quotechar='\"'))\n",
    "    \n",
    "    # Extract the headers (column names) from the first row\n",
    "    headers = rows[0]\n",
    "\n",
    "    # Find the indices of the target columns within the headers\n",
    "    target_col_indices = [headers.index(col) for col in target_cols]\n",
    "\n",
    "    # Iterate over each row in the CSV file, starting from the second row\n",
    "    for row in rows[1:]:\n",
    "        # Extract the relevant column values from each row using the indices\n",
    "        userId, sessionId, itemInSession, song, artist, firstName, lastName = [\n",
    "            row[i] for i in target_col_indices\n",
    "        ]\n",
    "\n",
    "        # Execute the SQL INSERT query with the extracted values\n",
    "        session.execute(\n",
    "            insert_query,(\n",
    "                int(userId),\n",
    "                int(sessionId),\n",
    "                int(itemInSession),\n",
    "                song,\n",
    "                artist,\n",
    "                firstName + ' ' + lastName,  # Concatenate the first name and last name together to form the complete name\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print a message indicating that data loading is complete\n",
    "    print(f'Data successfully loaded to the artist_table!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a SELECT query to confirm that the data has been successfully inserted into the `artist_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Down To The Bone', \"Keep On Keepin' On\", 'Sylvie Cruz')\n",
      "('Three Drives', 'Greece 2000', 'Sylvie Cruz')\n",
      "('Sebastien Tellier', 'Kilometer', 'Sylvie Cruz')\n",
      "('Lonnie Gordon', 'Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', 'Sylvie Cruz')\n"
     ]
    }
   ],
   "source": [
    "# Run a SELECT query to verify the data was loaded into the table\n",
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT artist, song, user \n",
    "        FROM artist_table \n",
    "        WHERE userId = 10 AND sessionId = 182\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print((row.artist, row.song, row.user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling for Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "```SQL\n",
    "SELECT user \n",
    "FROM user_table\n",
    "WHERE song = 'All Hands Against His Own'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be **partitioned** by `song`, with `userId` as the **clustering** column, to uniquely identify the users who listen to the same song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Table named `user_table` based on the Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user_table for Query 3\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS user_table (\n",
    "            song            text,\n",
    "            userId          int,\n",
    "            user            text,\n",
    "            PRIMARY KEY ((song), userId)\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data into `user_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded to the user_table!\n"
     ]
    }
   ],
   "source": [
    "# Specify the columns we want to extract from the CSV file\n",
    "target_cols = ['song', 'userId', 'firstName', 'lastName']\n",
    "\n",
    "# Define the SQL INSERT query to insert data into the 'user_table'\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO user_table (song, userId, user)\n",
    "    VALUES\n",
    "    (%s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Open the consolidated CSV file in read mode with UTF-8 encoding\n",
    "with open('event_datafile_new.csv', 'r', encoding='utf-8') as file:\n",
    "    # Read all rows from the CSV file into a list\n",
    "    rows = list(csv.reader(file, delimiter=',', quotechar='\"'))\n",
    "    \n",
    "    # Extract the headers (column names) from the first row\n",
    "    headers = rows[0]\n",
    "\n",
    "    # Find the indices of the target columns within the headers\n",
    "    target_col_indices = [headers.index(col) for col in target_cols]\n",
    "\n",
    "    # Iterate over each row in the CSV file, starting from the second row\n",
    "    for row in rows[1:]:\n",
    "        # Extract the relevant column values from each row using the indices\n",
    "        song, userId, firstName, lastName = [\n",
    "            row[i] for i in target_col_indices\n",
    "        ]\n",
    "\n",
    "        # Execute the SQL INSERT query with the extracted values\n",
    "        session.execute(\n",
    "            insert_query,(\n",
    "                song,\n",
    "                int(userId),\n",
    "                firstName + ' ' + lastName,   # Concatenate the first name and last name together to form the complete name\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print a message indicating that data loading is complete\n",
    "    print(f'Data successfully loaded to the user_table!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a SELECT query to confirm that the data has been successfully inserted into the `user_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "# Run a SELECT query to verify the data was loaded into the table\n",
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT user\n",
    "        FROM user_table\n",
    "        WHERE song = 'All Hands Against His Own'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print((row.user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the Tables Before Closing Out the Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all tables before closing out the sessions\n",
    "try:\n",
    "    session.execute(\"\"\"DROP TABLE IF EXISTS song_table\"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(\"\"\"DROP TABLE IF EXISTS artist_table\"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(\"\"\"DROP TABLE IF EXISTS user_table\"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Session and Cluster ConnectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the session to close the connection to the Cassandra database\n",
    "session.shutdown()\n",
    "# Shut down the cluster to close all connections associated with it\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
